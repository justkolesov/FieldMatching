{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff339354",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torchvision.utils import make_grid\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, TensorDataset, Subset\n",
    "from torchvision.datasets import ImageFolder\n",
    "\n",
    "from scipy import integrate, linalg\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    " \n",
    "\n",
    "import functools\n",
    "import os\n",
    "import math\n",
    "import functools\n",
    "import string\n",
    "import wandb\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.utils import Config\n",
    "\n",
    "\n",
    "from src.fid_score import calculate_frechet_distance, get_loader_stats\n",
    "from src.inception import InceptionV3\n",
    "from src.utils import freeze, unfreeze\n",
    "import gc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64ea0f39",
   "metadata": {},
   "source": [
    "## 1. Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6475778",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = Config()\n",
    "\n",
    "config.device = 'cuda'\n",
    "\n",
    "config.data = Config()\n",
    "config.data.num_channels = 3\n",
    "config.data.channels = 3\n",
    "config.data.centered = True\n",
    "config.data.img_resize=32\n",
    "config.data.image_size =32\n",
    "\n",
    "\n",
    "config.training = Config()\n",
    "config.training.sde = 'poisson'\n",
    "config.training.continuous = True\n",
    "config.training.batch_size = 128#4096\n",
    "config.training.small_batch_size = 128\n",
    "config.training.gamma = 5\n",
    "config.training.restrict_M = True\n",
    "config.training.tau = 0.03\n",
    "config.training.snapshot_freq = 5_000\n",
    "config.training.eval_freq = 5_000\n",
    "config.training.model = 'ddpmpp'\n",
    "config.training.M = 291\n",
    "config.training.reduce_mean = False\n",
    "config.training.n_iters =  1_000_000\n",
    "config.training.fid_freq = 25_000\n",
    "config.training.fid_batch_size = 250\n",
    "\n",
    "config.model  = Config()\n",
    "config.model.name = 'ncsnpp'\n",
    "config.model.scale_by_sigma = False\n",
    "config.model.ema_rate = 0.9999\n",
    "config.model.normalization = 'GroupNorm'\n",
    "config.model.nonlinearity = 'swish'\n",
    "config.model.nf = 128\n",
    "config.model.ch_mult = (1, 2, 2, 2)\n",
    "config.model.num_res_blocks = 4\n",
    "config.model.attn_resolutions = (16,)\n",
    "config.model.resamp_with_conv = True\n",
    "config.model.conditional = True\n",
    "config.model.fir = False\n",
    "config.model.fir_kernel = [1, 3, 3, 1]\n",
    "config.model.skip_rescale = True\n",
    "config.model.resblock_type = 'biggan'\n",
    "config.model.progressive = 'none'\n",
    "config.model.progressive_input = 'none'\n",
    "config.model.progressive_combine = 'sum'\n",
    "config.model.attention_type = 'ddpm'\n",
    "config.model.init_scale = 0.\n",
    "config.model.fourier_scale = 16\n",
    "config.model.embedding_type = 'positional'\n",
    "config.model.conv_size = 3\n",
    "config.model.sigma_end = 0.01\n",
    "config.model.dropout = 0.1\n",
    "\n",
    "config.optim  = Config()\n",
    "config.optim.weight_decay = 0\n",
    "config.optim.optimizer = 'Adam'\n",
    "config.optim.lr = 2e-7\n",
    "config.optim.beta1 = 0.9\n",
    "config.optim.eps = 1e-8\n",
    "config.optim.warmup = 5000\n",
    "config.optim.grad_clip = 1.\n",
    "\n",
    "\n",
    "config.device = 'cuda'\n",
    "\n",
    "config.sampling = Config()\n",
    "config.sampling.method = 'ode'\n",
    "config.sampling.ode_solver = 'rk45'\n",
    "config.sampling.N = 100\n",
    "config.sampling.z_max = 30\n",
    "config.sampling.z_min = 1e-7\n",
    "config.sampling.upper_norm = 3000\n",
    "config.sampling.z_exp=1\n",
    "config.sampling.visual_iterations = 10\n",
    "# verbose\n",
    "config.sampling.vs = False\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696df3c3",
   "metadata": {},
   "source": [
    "## 2. Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08b05dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sampler:\n",
    "    def __init__(\n",
    "        self, device='cuda',\n",
    "    ):\n",
    "        self.device = device\n",
    "    \n",
    "    def sample(self, size=5):\n",
    "        pass\n",
    "    \n",
    "class LoaderSampler(Sampler):\n",
    "    def __init__(self, loader, device='cuda'):\n",
    "        super(LoaderSampler, self).__init__(device)\n",
    "        self.loader = loader\n",
    "        self.it = iter(self.loader)\n",
    "        \n",
    "    def sample(self, size=5):\n",
    "        assert size <= self.loader.batch_size\n",
    "        try:\n",
    "            batch, _ = next(self.it)\n",
    "        except StopIteration:\n",
    "            self.it = iter(self.loader)\n",
    "            return self.sample(size)\n",
    "        if len(batch) < size:\n",
    "            return self.sample(size)\n",
    "            \n",
    "        return batch[:size].to(self.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a4ba97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_colored_images(images, seed = 0x000000):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    images = 0.5*(images + 1)\n",
    "    size = images.shape[0]\n",
    "    colored_images = []\n",
    "    hues = 360*np.random.rand(size)\n",
    "    \n",
    "    for V, H in zip(images, hues):\n",
    "        V_min = 0\n",
    "        \n",
    "        a = (V - V_min)*(H%60)/60\n",
    "        V_inc = a\n",
    "        V_dec = V - a\n",
    "        \n",
    "        colored_image = torch.zeros((3, V.shape[1], V.shape[2]))\n",
    "        H_i = round(H/60) % 6\n",
    "        \n",
    "        if H_i == 0:\n",
    "            colored_image[0] = V\n",
    "            colored_image[1] = V_inc\n",
    "            colored_image[2] = V_min\n",
    "        elif H_i == 1:\n",
    "            colored_image[0] = V_dec\n",
    "            colored_image[1] = V\n",
    "            colored_image[2] = V_min\n",
    "        elif H_i == 2:\n",
    "            colored_image[0] = V_min\n",
    "            colored_image[1] = V\n",
    "            colored_image[2] = V_inc\n",
    "        elif H_i == 3:\n",
    "            colored_image[0] = V_min\n",
    "            colored_image[1] = V_dec\n",
    "            colored_image[2] = V\n",
    "        elif H_i == 4:\n",
    "            colored_image[0] = V_inc\n",
    "            colored_image[1] = V_min\n",
    "            colored_image[2] = V\n",
    "        elif H_i == 5:\n",
    "            colored_image[0] = V\n",
    "            colored_image[1] = V_min\n",
    "            colored_image[2] = V_dec\n",
    "        \n",
    "        colored_images.append(colored_image)\n",
    "        \n",
    "    colored_images = torch.stack(colored_images, dim = 0)\n",
    "    colored_images = 2*colored_images - 1\n",
    "    \n",
    "    return colored_images\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0abd42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(name, path, img_size=64, batch_size=64, \n",
    "                 shuffle=True, device='cuda', return_dataset=False,\n",
    "                 num_workers=0):\n",
    "    \n",
    "    \n",
    "    if name.startswith(\"MNIST\"):\n",
    "        # In case of using certain classe from the MNIST dataset you need to specify them by writing in the next format \"MNIST_{digit}_{digit}_..._{digit}\"\n",
    "        transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.Resize((32, 32)),\n",
    "            torchvision.transforms.ToTensor(),\n",
    "            torchvision.transforms.Lambda(lambda x: 2 * x - 1)\n",
    "        ])\n",
    "        \n",
    "        dataset_name = name.split(\"_\")[0]\n",
    "        is_colored = dataset_name[-7:] == \"colored\"\n",
    "        \n",
    "        classes = [int(number) for number in name.split(\"_\")[1:]]\n",
    "        if not classes:\n",
    "            classes = [i for i in range(10)]\n",
    "        \n",
    "        train_set = torchvision.datasets.MNIST(path, train=True, transform=transform, download=True)\n",
    "        test_set = torchvision.datasets.MNIST(path, train=False, transform=transform, download=True)\n",
    "        \n",
    "        train_test = []\n",
    "        \n",
    "        for dataset in [train_set, test_set]:\n",
    "            data = []\n",
    "            labels = []\n",
    "            for k in range(len(classes)):\n",
    "                data.append(torch.stack(\n",
    "                    [dataset[i][0] for i in range(len(dataset.targets)) if dataset.targets[i] == classes[k]],\n",
    "                    dim=0\n",
    "                ))\n",
    "                labels += [k]*data[-1].shape[0]\n",
    "            data = torch.cat(data, dim=0)\n",
    "            data = data.reshape(-1, 1, 32, 32)\n",
    "            labels = torch.tensor(labels)\n",
    "            \n",
    "            if is_colored:\n",
    "                data = get_random_colored_images(data)\n",
    "            \n",
    "            train_test.append(TensorDataset(data, labels))\n",
    "            \n",
    "        train_set, test_set = train_test  \n",
    "    else:\n",
    "        raise Exception('Unknown dataset')\n",
    "    \n",
    "    if return_dataset:\n",
    "        return train_set, test_set\n",
    "        \n",
    "    train_sampler = LoaderSampler(DataLoader(train_set, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size), device)\n",
    "    test_sampler = LoaderSampler(DataLoader(test_set, shuffle=shuffle, num_workers=num_workers, batch_size=batch_size), device)\n",
    "    return train_sampler, test_sampler, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7a820d",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATASET1, DATASET1_PATH = 'MNIST-colored', '../data/MNIST'\n",
    " \n",
    "X_sampler, X_test_sampler, Dataset = load_dataset(DATASET1, DATASET1_PATH, \n",
    "                                         img_size=config.data.image_size,\n",
    "                                         batch_size=config.training.batch_size, num_workers=8)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9998b1",
   "metadata": {},
   "source": [
    "## 3. Poisson class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae7fcc2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Poisson():\n",
    "    \n",
    "    \n",
    "    def __init__(self, config):\n",
    "        \"\"\"Construct a PFGM.\n",
    "\n",
    "        Args:\n",
    "          config: configurations\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        self.N = config.sampling.N\n",
    "\n",
    "        \n",
    "    @property\n",
    "    def M(self):\n",
    "        return self.config.training.M\n",
    "\n",
    " \n",
    "        \n",
    "    def ode(self, net_fn, x, t):\n",
    "\n",
    "        z = np.exp(t.mean().cpu())\n",
    "        if self.config.sampling.vs:\n",
    "            print(z)\n",
    "        x_drift, z_drift = net_fn(x, torch.ones((len(x))).cuda() * z)\n",
    "        x_drift = x_drift.view(len(x_drift), -1)\n",
    "\n",
    "        # Substitute the predicted z with the ground-truth\n",
    "        # Please see Appendix B.2.3 in PFGM paper (https://arxiv.org/abs/2209.11178) for details\n",
    "        z_exp = self.config.sampling.z_exp\n",
    "        if z < z_exp and self.config.training.gamma > 0:\n",
    "            data_dim = self.config.data.image_size * self.config.data.image_size * self.config.data.channels\n",
    "            sqrt_dim = np.sqrt(data_dim)\n",
    "            norm_1 = x_drift.norm(p=2, dim=1) / sqrt_dim\n",
    "            x_norm = self.config.training.gamma * norm_1 / (1 -norm_1)\n",
    "            x_norm = torch.sqrt(x_norm ** 2 + z ** 2)\n",
    "            z_drift = -sqrt_dim * torch.ones_like(z_drift) * z / (x_norm + self.config.training.gamma)\n",
    "\n",
    "        # Predicted normalized Poisson field\n",
    "        v = torch.cat([x_drift, z_drift[:, None]], dim=1)\n",
    "\n",
    "        dt_dz = 1 / (v[:, -1] + 1e-5)\n",
    "        dx_dt = v[:, :-1].view(len(x), self.config.data.num_channels,\n",
    "                          self.config.data.image_size, self.config.data.image_size)\n",
    "        dx_dz = dx_dt * dt_dz.view(-1, *([1] * len(x.size()[1:])))\n",
    "        # dx/dt_prime =  z * dx/dz\n",
    "        dx_dt_prime = z * dx_dz\n",
    "        return dx_dt_prime"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5073eedf",
   "metadata": {},
   "source": [
    "## 4. Utils Poisson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fc61d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_pz(sde, config,  samples_batch_x, samples_batch_y):\n",
    "    \n",
    "    \"\"\"Perturbing the augmented training data. See Algorithm 2 in PFGM paper.\n",
    "\n",
    "    Args:\n",
    "      sde: An `methods.SDE` object that represents the forward SDE.\n",
    "      config: configurations\n",
    "      samples_batch: A mini-batch of un-augmented training data\n",
    "      m: A 1D torch tensor. The exponents of (1+\\tau).\n",
    "\n",
    "    Returns:\n",
    "      Perturbed samples\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \"\"\"\n",
    "    m = torch.rand((samples_batch_x.shape[0],), device=samples_batch_x.device) * sde.M\n",
    "    #### noise parametrization ####\n",
    "    tau = config.training.tau\n",
    "    z = torch.randn((len(samples_batch_x), 1, 1, 1)).to(samples_batch_x.device) * config.model.sigma_end\n",
    "    z = z.abs()\n",
    "    data_dim = config.data.channels * config.data.image_size * config.data.image_size\n",
    "    multiplier = (1+tau) ** m\n",
    "    perturbed_z = z.squeeze() * multiplier\n",
    "    \n",
    "    perturbed_x = samples_batch_x*(perturbed_z[0]/config.sampling.z_max) + (1 - perturbed_z[0]/config.sampling.z_max)*samples_batch_y\n",
    "    perturbed_x += torch.randn_like(perturbed_x)\n",
    "    perturbed_samples_vec = torch.cat((perturbed_x.reshape(len(samples_batch_x), -1),\n",
    "                                       perturbed_z[:, None]), dim=1)\n",
    "    return perturbed_samples_vec\n",
    "    #### noise parametrization ####\n",
    "    \"\"\"\n",
    " \n",
    " \n",
    "    m = torch.rand((samples_batch_x.shape[0],), device=samples_batch_x.device) * sde.M\n",
    "    data_dim = config.data.channels * config.data.image_size * config.data.image_size # N\n",
    "    tau = config.training.tau\n",
    "    z = torch.randn((len(samples_batch_x), 1, 1, 1)).to(samples_batch_x.device) * config.model.sigma_end  # [B,1,1,1]\n",
    "    z = z.abs() # [B,1,1,1]\n",
    "    \n",
    "    \n",
    "    # Confine the norms of perturbed data.\n",
    "    # see Appendix B.1.1 of https://arxiv.org/abs/2209.11178\n",
    "    if config.training.restrict_M:\n",
    "        idx = (z < 0.005).squeeze()\n",
    "        num = int(idx.int().sum())\n",
    "        restrict_m = int(sde.M * 0.7)\n",
    "        m[idx] = torch.rand((num,), device=samples_batch_x.device) * restrict_m\n",
    "    \n",
    "        \n",
    "    multiplier = (1+tau) ** m # torch.Size([B])\n",
    "    # Perturb z\n",
    "    perturbed_z = z.squeeze() * multiplier # torch.Size([B])* torch.Size([B]) = torch.Size([B])\n",
    "    \n",
    "    \n",
    "    ####### perturbation for x component #######\n",
    "    \n",
    "    # Sample uniform angle\n",
    "    gaussian = torch.randn(len(samples_batch_x), data_dim).to(samples_batch_x.device) # torch.Size([B, C*H*W])\n",
    "    unit_gaussian = gaussian / torch.norm(gaussian, p=2, dim=1, keepdim=True) #  torch.Size([B, C*H*W])\n",
    "    \n",
    "    # injected noise amount\n",
    "    noise = torch.randn_like(samples_batch_x).reshape(len(samples_batch_x), -1) * config.model.sigma_end #torch.Size([B, C*H*W])\n",
    "    norm_m = torch.norm(noise, p=2, dim=1) * multiplier # torch.Size([B])*torch.Size([B]) = torch.Size([B])\n",
    "    \n",
    "    \n",
    "    # Construct the perturbation for x\n",
    "    perturbation_x = unit_gaussian * norm_m[:, None] # torch.Size([B,C*H*W])* torch.Size([B,1])=  torch.Size([B,C*H*W])\n",
    "    perturbation_x = perturbation_x.view_as(samples_batch_x) # torch.size([B,C,H,W])\n",
    "    \n",
    "    # Perturb x\n",
    "    perturbed_x = samples_batch_x + perturbation_x # torch.size([B,C,H,W])\n",
    "    \n",
    "    # Augment the data with extra dimension z\n",
    "    perturbed_samples_vec = torch.cat((perturbed_x.reshape(len(samples_batch_x), -1),\n",
    "                                       perturbed_z[:, None]), dim=1)\n",
    "    \n",
    "    # concatenate: torch.Size([B,C*H*W], torch.Size([[B,1]]) = torch.Size([B,C*H*W + 1]\n",
    "    return perturbed_samples_vec\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52975da6",
   "metadata": {},
   "source": [
    "## 5. losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b206eb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_pfgm(model, batch_x, batch_y):\n",
    "    \"\"\"Compute the loss function.\n",
    "\n",
    "    Args:\n",
    "      model: A PFGM or score model.\n",
    "      batch: A mini-batch of training data.\n",
    "\n",
    "    Returns:\n",
    "      loss: A scalar that represents the average loss value across the mini-batch.\n",
    "    \"\"\"\n",
    "    samples_full_x = batch_x\n",
    "    samples_full_y = batch_y\n",
    "\n",
    "    perturbed_samples_vec = forward_pz(sde, sde.config,batch_x, batch_y )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        \n",
    "        real_samples_vec_x = torch.cat(\n",
    "          (samples_full_x.reshape(len(samples_full_x), -1),\n",
    "           torch.zeros((len(samples_full_x), 1)).to(samples_full_x.device)), dim=1)\n",
    "        \n",
    "        real_samples_vec_y = torch.cat(\n",
    "          (samples_full_y.reshape(len(samples_full_y), -1),\n",
    "           config.sampling.z_max*torch.ones((len(samples_full_y), 1)).to(samples_full_y.device)), dim=1)\n",
    "\n",
    "        data_dim = sde.config.data.image_size * sde.config.data.image_size * sde.config.data.channels\n",
    "        \n",
    "        gt_distance_x = torch.sum((perturbed_samples_vec.unsqueeze(1) - real_samples_vec_x) ** 2,\n",
    "                                dim=[-1]).sqrt()\n",
    "        gt_distance_y = torch.sum((perturbed_samples_vec.unsqueeze(1) - real_samples_vec_y) ** 2,\n",
    "                                dim=[-1]).sqrt()\n",
    "        \n",
    "\n",
    "        # For numerical stability, timing each row by its minimum value\n",
    "        distance_x = torch.min(gt_distance_x, dim=1, keepdim=True)[0] / (gt_distance_x + 1e-7)\n",
    "        distance_x = distance_x ** (data_dim + 1)\n",
    "        distance_x = distance_x[:, :, None]\n",
    "        \n",
    "        distance_y = torch.min(gt_distance_y, dim=1, keepdim=True)[0] / (gt_distance_y + 1e-7)\n",
    "        distance_y = distance_y ** (data_dim + 1)\n",
    "        distance_y = distance_y[:, :, None]\n",
    "\n",
    "\n",
    "        # Normalize the coefficients (effectively multiply by c(\\tilde{x}) in the paper)\n",
    "        coeff_x = distance_x / (torch.sum(distance_x, dim=1, keepdim=True) + 1e-7)\n",
    "        coeff_y = distance_y / (torch.sum(distance_y, dim=1, keepdim=True) + 1e-7)\n",
    "        \n",
    "        diff_x = - (perturbed_samples_vec.unsqueeze(1) - real_samples_vec_x)\n",
    "        diff_y = - (perturbed_samples_vec.unsqueeze(1) - real_samples_vec_y)\n",
    "\n",
    "        # Calculate empirical Poisson field (N+1 dimension in the augmented space)\n",
    "        gt_direction_x = torch.sum(coeff_x * diff_x, dim=1)\n",
    "        gt_direction_x = gt_direction_x.view(gt_direction_x.size(0), -1)\n",
    "        \n",
    "        gt_direction_y = torch.sum(coeff_y * diff_y, dim=1)\n",
    "        gt_direction_y = gt_direction_y.view(gt_direction_y.size(0), -1)\n",
    "\n",
    "\n",
    "    gt_norm_x = gt_direction_x.norm(p=2, dim=1)\n",
    "    # Normalizing the N+1-dimensional Poisson field\n",
    "    gt_direction_x /= (gt_norm_x.view(-1, 1) + sde.config.training.gamma)\n",
    "    gt_direction_x *= np.sqrt(data_dim)\n",
    "    \n",
    "    gt_norm_y = gt_direction_y.norm(p=2, dim=1)\n",
    "    # Normalizing the N+1-dimensional Poisson field\n",
    "    gt_direction_y /= (gt_norm_y.view(-1, 1) + sde.config.training.gamma)\n",
    "    gt_direction_y *= np.sqrt(data_dim)\n",
    "\n",
    "\n",
    "    target = gt_direction_x - gt_direction_y\n",
    "\n",
    "    #net_fn = mutils.get_predict_fn(sde, model, train=train, continuous=continuous)\n",
    "\n",
    "    perturbed_samples_x = perturbed_samples_vec[:, :-1].view_as(batch_x)\n",
    "    #perturbed_samples_z = torch.clamp(perturbed_samples_vec[:, -1], 1e-10)\n",
    "    perturbed_samples_z = perturbed_samples_vec[:, -1]\n",
    "    net_x, net_z = model(perturbed_samples_x, perturbed_samples_z)\n",
    "\n",
    "    net_x = net_x.view(net_x.shape[0], -1)\n",
    "    # Predicted N+1-dimensional Poisson field\n",
    "    net = torch.cat([net_x, net_z[:, None]], dim=1)\n",
    "    loss = ((net - target) ** 2)\n",
    "    #loss = reduce_op(loss.reshape(loss.shape[0], -1), dim=-1)\n",
    "    loss = torch.mean(loss)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2958e78",
   "metadata": {},
   "source": [
    "## 6. ODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be2cdb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rk45_sampler_pfgm(sde, y, config, shape,   rtol=1e-4, atol=1e-4,\n",
    "                    method='RK45', eps=1e-3, device='cuda'):\n",
    "\n",
    "    \"\"\"RK45 ODE sampler for PFGM.\n",
    "\n",
    "    Args:\n",
    "    sde: An `methods.SDE` object that represents PFGM.\n",
    "    shape: A sequence of integers. The expected shape of a single sample.\n",
    "    inverse_scaler: The inverse data normalizer.\n",
    "    rtol: A `float` number. The relative tolerance level of the ODE solver.\n",
    "    atol: A `float` number. The absolute tolerance level of the ODE solver.\n",
    "    method: A `str`. The algorithm used for the black-box ODE solver.\n",
    "      See the documentation of `scipy.integrate.solve_ivp`.\n",
    "    eps: A `float` number. The reverse-time SDE/ODE will be integrated to `eps` for numerical stability.\n",
    "    device: PyTorch device.\n",
    "\n",
    "    Returns:\n",
    "    A sampling function that returns samples and the number of function evaluations during sampling.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    def ode_sampler(model, y):\n",
    "\n",
    "        x = y\n",
    "\n",
    "        z = torch.ones((len(x), 1, 1, 1)).to(x.device)\n",
    "        z = z.repeat((1, 1, sde.config.data.image_size, sde.config.data.image_size)) * sde.config.sampling.z_max\n",
    "        x = x.view(shape)\n",
    "        # Augment the samples with extra dimension z\n",
    "        # We concatenate the extra dimension z as an addition channel to accomondate this solver\n",
    "        x = torch.cat((x, z), dim=1)\n",
    "        x = x.float()\n",
    "        new_shape = (len(x), sde.config.data.channels + 1, sde.config.data.image_size, sde.config.data.image_size)\n",
    "        \n",
    "        \n",
    "\n",
    "        def ode_func(t, x):\n",
    "\n",
    "            if sde.config.sampling.vs:\n",
    "                print(np.exp(t))\n",
    "\n",
    "\n",
    "            x = from_flattened_numpy(x, new_shape).to(device).type(torch.float32)\n",
    "\n",
    "            # Change-of-variable z=exp(t)\n",
    "            z = np.exp(t)\n",
    "            #net_fn = get_predict_fn(sde, model, train=False)\n",
    "\n",
    "            x_drift, z_drift = model(x[:, :-1], torch.ones((len(x))).cuda() * z)\n",
    "            x_drift = x_drift.view(len(x_drift), -1)\n",
    "\n",
    "            # Substitute the predicted z with the ground-truth\n",
    "            # Please see Appendix B.2.3 in PFGM paper (https://arxiv.org/abs/2209.11178) for details\n",
    "            z_exp = sde.config.sampling.z_exp\n",
    "\n",
    "\n",
    "\n",
    "            if z < z_exp and sde.config.training.gamma > 0:\n",
    "                data_dim = sde.config.data.image_size * sde.config.data.image_size * sde.config.data.channels\n",
    "                sqrt_dim = np.sqrt(data_dim)\n",
    "                norm_1 = x_drift.norm(p=2, dim=1) / sqrt_dim\n",
    "                x_norm = sde.config.training.gamma * norm_1 / (1 - norm_1)\n",
    "                x_norm = torch.sqrt(x_norm ** 2 + z ** 2)\n",
    "                z_drift = -sqrt_dim * torch.ones_like(z_drift) * z / (x_norm + sde.config.training.gamma)\n",
    "\n",
    "                \n",
    "                \n",
    "            # Predicted normalized Poisson field\n",
    "            v = torch.cat([x_drift, z_drift[:, None]], dim=1)\n",
    "            dt_dz = 1 / (v[:, -1] + 1e-5)\n",
    "            dx_dt = v[:, :-1].view(shape)\n",
    "\n",
    "            # Get dx/dz\n",
    "            dx_dz = dx_dt * dt_dz.view(-1, *([1] * len(x.size()[1:])))\n",
    "            # drift = z * (dx/dz, dz/dz) = z * (dx/dz, 1)\n",
    "            drift = torch.cat([z * dx_dz,\n",
    "                               torch.ones((len(dx_dz), 1, sde.config.data.image_size,\n",
    "                                           sde.config.data.image_size)).to(dx_dz.device) * z], dim=1)\n",
    "            return to_flattened_numpy(drift)\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Black-box ODE solver for the probability flow ODE.\n",
    "        # Note that we use z = exp(t) for change-of-variable to accelearte the ODE simulation\n",
    "        solution = integrate.solve_ivp(ode_func,\n",
    "                                       (np.log(sde.config.sampling.z_max),\n",
    "                                                  np.log(eps)), to_flattened_numpy(x),\n",
    "                                     rtol=rtol, atol=atol, method=method)\n",
    "\n",
    "        nfe = solution.nfev\n",
    "        num_itrs = len(solution.y[0])\n",
    "        x = torch.tensor(solution.y[:, -1]).reshape(new_shape).to(device).type(torch.float32)\n",
    "        \n",
    "        trajectory = []\n",
    "        visual_iters = np.linspace(int(num_itrs//8), num_itrs, config.sampling.visual_iterations)\n",
    "       \n",
    "        for itr in visual_iters:\n",
    "            traj = torch.tensor(solution.y[:,int(itr)-1]).reshape(new_shape).to(device).type(torch.float32)\n",
    "            trajectory.append(traj[:,:-1])\n",
    "            \n",
    "            \n",
    "        # Detach augmented z dimension\n",
    "        x = x[:, :-1]\n",
    "        #x = inverse_scaler(x)\n",
    "        return x, nfe, torch.stack(trajectory,dim=0)\n",
    "\n",
    "    return ode_sampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3db16cfd",
   "metadata": {},
   "source": [
    "## 7. Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82405630",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimization_manager(config):\n",
    "    \"\"\"Returns an optimize_fn based on `config`.\"\"\"\n",
    "\n",
    "    def optimize_fn(optimizer, params, step, lr=config.optim.lr,\n",
    "                      warmup=config.optim.warmup,\n",
    "                      grad_clip=config.optim.grad_clip):\n",
    "        \"\"\"Optimizes with warmup and gradient clipping (disabled if negative).\"\"\"\n",
    "        if warmup > 0:\n",
    "            for g in optimizer.param_groups:\n",
    "                g['lr'] = lr * np.minimum(step / warmup, 1.0)\n",
    "        if grad_clip >= 0:\n",
    "            torch.nn.utils.clip_grad_norm_(params, max_norm=grad_clip)\n",
    "        optimizer.step()\n",
    "\n",
    "    return optimize_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a09de5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "sys.path.append(\"..\")\n",
    "from models import DDPM, ExponentialMovingAverage\n",
    "net = DDPM(config).to(config.device)\n",
    "\n",
    "params = net.parameters()\n",
    "optimizer = torch.optim.Adam(params,\n",
    "                       lr=config.optim.lr, betas=(config.optim.beta1, 0.999), eps=config.optim.eps,\n",
    "                       weight_decay=config.optim.weight_decay)\n",
    "\n",
    "ema = ExponentialMovingAverage(net.parameters(), decay=config.model.ema_rate)\n",
    "state = dict(optimizer=optimizer, model=net, ema=ema, step=0)\n",
    "sde = Poisson(config=config)\n",
    "sampling_eps = config.sampling.z_min\n",
    "optimize_fn = optimization_manager(config)\n",
    "reduce_mean = config.training.reduce_mean\n",
    "\n",
    "num_train_steps = config.training.n_iters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18070850",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_flattened_numpy(x):\n",
    "    \"\"\"Flatten a torch tensor `x` and convert it to numpy.\"\"\"\n",
    "    return x.detach().cpu().numpy().reshape((-1,))\n",
    "\n",
    "\n",
    "def from_flattened_numpy(x, shape):\n",
    "    \"\"\"Form a torch tensor with the given `shape` from a flattened numpy array `x`.\"\"\"\n",
    "    return torch.from_numpy(x.reshape(shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e427ab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(x):\n",
    "    fig,ax = plt.subplots(5,5,figsize=(5,5))\n",
    "    for idx in range(5):\n",
    "        for jdx in range(5):\n",
    "            ax[idx,jdx].imshow(x[idx,jdx])\n",
    "            ax[idx,jdx].set_yticks([])\n",
    "            ax[idx,jdx].set_xticks([])\n",
    "    fig.tight_layout(pad=0.001)       \n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce1ddcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_trajectory(traj):\n",
    "    \n",
    "    fig,ax = plt.subplots(5,len(traj),figsize=(len(traj),5),sharex=True,sharey=True)\n",
    "    for time in range(len(traj)):\n",
    "        for idx in range(5):\n",
    "            ax[idx,time].imshow(np.clip(traj[time,idx].permute(1,2,0).cpu().numpy()*255,0,255).astype(np.uint8))\n",
    "            ax[idx,time].set_xticks([])\n",
    "            ax[idx,time].set_yticks([])\n",
    "\n",
    "    fig.tight_layout(pad=0.01)\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0ae983f",
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = DataLoader(Dataset, shuffle=True, num_workers=8, batch_size=config.training.fid_batch_size)\n",
    "mu_data,sigma_data = get_loader_stats(loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe664aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pushed_loader_stats(net, sde, config, batch_size, verbose=False, device='cuda',\n",
    "                            use_downloaded_weights=False):\n",
    "    dims = 2048\n",
    "    block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[dims]\n",
    "    model = InceptionV3([block_idx], use_downloaded_weights=use_downloaded_weights).to(device)\n",
    "    freeze(net); freeze(model);\n",
    "    \n",
    "    size = len(loader.dataset)\n",
    "    pred_arr = []\n",
    "    shape = (config.training.fid_batch_size, config.data.num_channels,\n",
    "                         config.data.image_size, config.data.image_size)\n",
    "     \n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for step, (X, _) in tqdm(enumerate(loader)) if not verbose else tqdm(enumerate(loader)):\n",
    "            for i in range(0, len(X), batch_size):\n",
    "                start, end = i, min(i + batch_size, len(X))\n",
    "                \n",
    "                batch_y = torch.randn(config.training.fid_batch_size, config.data.num_channels,\n",
    "                                  config.data.image_size,config.data.image_size)\n",
    "                sampling_fn = get_rk45_sampler_pfgm(sde=sde, y=batch_y , config=config,\n",
    "                                               shape=shape,\n",
    "                                               eps=config.sampling.z_min,\n",
    "                                               device=config.device)\n",
    "    \n",
    "                batch,_,_ = sampling_fn(net, batch_y)\n",
    "        \n",
    "                pred_arr.append(model(batch)[0].cpu().data.numpy().reshape(end-start, -1))\n",
    "\n",
    "    pred_arr = np.vstack(pred_arr[:-1])\n",
    "    mu, sigma = np.mean(pred_arr, axis=0), np.cov(pred_arr, rowvar=False)\n",
    "    gc.collect(); torch.cuda.empty_cache()\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ebe364",
   "metadata": {},
   "source": [
    "##  8. Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9e0783",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "wandb.init(project=\"ElectroGeneration\",\n",
    "name=f\"CM_Exp_Decay_BS_{config.training.batch_size}_LR_{config.optim.lr}_zmax_{config.sampling.z_max}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d02b995",
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_step = 0\n",
    " \n",
    "for step in tqdm(range(initial_step, num_train_steps + 1)):\n",
    "    \n",
    "    batch_x = X_sampler.sample(config.training.batch_size).to(config.device)\n",
    "    #batch_y = Y_sampler.sample(config.training.batch_size).to(config.device)\n",
    "    batch_y = torch.randn_like(batch_x).to(config.device)\n",
    "     \n",
    "    \n",
    "    optimizer = state['optimizer']\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    loss = loss_pfgm(net, batch_x, batch_y)\n",
    "    loss.backward()\n",
    "    optimize_fn(optimizer, net.parameters(), step=state['step'])\n",
    "    state['step'] += 1\n",
    "    state['ema'].update(net.parameters())\n",
    "    wandb.log({\"loss train\":loss.item()},step=step)\n",
    "    \n",
    "   \n",
    "    if step % config.training.eval_freq == 0:\n",
    "        \n",
    "        batch_x = X_test_sampler.sample(config.training.batch_size).to(config.device)\n",
    "        #batch_y = Y_test_sampler.sample(config.training.batch_size).to(config.device)\n",
    "        batch_y = torch.randn_like(batch_x).to(config.device)\n",
    "        \n",
    "            \n",
    "        with torch.no_grad():\n",
    "            ema = state['ema']\n",
    "            ema.store(net.parameters())\n",
    "            ema.copy_to(net.parameters())\n",
    "            eval_loss = loss_pfgm(net, batch_x, batch_y)\n",
    "            ema.restore(net.parameters())\n",
    "            wandb.log({\"loss eval\":eval_loss.item()},step=step)\n",
    "\n",
    " \n",
    " \n",
    "    if step % config.training.snapshot_freq == 0:\n",
    "        with torch.no_grad():\n",
    "            ema.store(net.parameters())\n",
    "            ema.copy_to(net.parameters())\n",
    "\n",
    "            shape = (25, config.data.num_channels,\n",
    "                         config.data.image_size, config.data.image_size)\n",
    "\n",
    "            batch_y = torch.randn(25, config.data.num_channels,\n",
    "                                  config.data.image_size,config.data.image_size)\n",
    "                                  \n",
    "                                  \n",
    "            sampling_fn = get_rk45_sampler_pfgm(sde=sde,\n",
    "                                                y=batch_y , config=config,\n",
    "                                               shape=shape,\n",
    "                                               eps=config.sampling.z_min,\n",
    "                                               device=config.device)\n",
    "            sample, n, traj = sampling_fn(net, batch_y)\n",
    "            ema.restore(net.parameters())\n",
    "            \n",
    "\n",
    "            sample = np.clip(sample.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)\n",
    "            batch_y = np.clip(batch_y.permute(0, 2, 3, 1).cpu().numpy() * 255, 0, 255).astype(np.uint8)\n",
    "            fig_1 = plot(sample.reshape(5,5,32,32,3) )\n",
    "            fig_2 = plot(batch_y.reshape(5,5,32,32,3) )\n",
    "            fig_3 = plot_trajectory(traj)\n",
    "            wandb.log({\"Generated Images\":fig_1},step=step)\n",
    "            wandb.log({\"Init Images\":fig_2},step=step)\n",
    "            wandb.log({\"Trajectories\":fig_3},step=step)\n",
    "            \n",
    "            \n",
    "    if step % config.training.fid_freq == 0 and step>1 :\n",
    "        with torch.no_grad():\n",
    "            mu,sigma = get_pushed_loader_stats(net, sde, config, batch_size=config.training.fid_batch_size, \n",
    "                                               verbose=False, device='cuda',\n",
    "                            use_downloaded_weights=False)\n",
    "            fid = calculate_frechet_distance(mu,sigma,mu_data,sigma_data)\n",
    "            unfreeze(net)\n",
    "            wandb.log({\"FID\":fid},step=step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b01142b8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
